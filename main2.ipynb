{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T21:54:44.477921Z",
     "start_time": "2024-11-29T21:54:44.472929Z"
    }
   },
   "outputs": [],
   "source": [
    "from MNISTDataset import MNISTDataset\n",
    "from CNN import CNNModel\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from torchvision import transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T21:54:45.298818Z",
     "start_time": "2024-11-29T21:54:45.041475Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(r\"C:\\School\\Term 7\\Introduction to Machine learning\\Lab\\Lab 2\\Neural-network\\archive\\train-images.idx3-ubyte\", r\"C:\\School\\Term 7\\Introduction to Machine learning\\Lab\\Lab 2\\Neural-network\\archive\\train-labels.idx1-ubyte\")\n",
    "test_dataset = MNISTDataset(r\"C:\\School\\Term 7\\Introduction to Machine learning\\Lab\\Lab 2\\Neural-network\\archive\\t10k-images.idx3-ubyte\", r\"C:\\School\\Term 7\\Introduction to Machine learning\\Lab\\Lab 2\\Neural-network\\archive\\t10k-labels.idx1-ubyte\")\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
    "\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "filter_config = [\n",
    "    [32, 64, 128],  \n",
    "    [64, 128, 256],  \n",
    "    [128, 256, 512]]\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1000, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "epochs = 5\n",
    "mymodel = CNNModel()\n",
    "criterion =nn.CrossEntropyLoss()\n",
    "optimize=torch.optim.SGD(mymodel.parameters(),lr=0.01)\n",
    "\n",
    "best_lr_config = {\"learning_rate\": None, \"val_accuracy\": 0}\n",
    "best_bs_config = {\"batch_size\": None, \"val_accuracy\": 0}\n",
    "best_filter_config = {\"filter_config\": None, \"val_accuracy\": 0}\n",
    "\n",
    "# Combined best configuration\n",
    "combined_best_config = {\"learning_rate\": None, \"batch_size\": None, \"filter_config\": None, \"val_accuracy\": 0}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST datasets for training, validation, and testing\n",
    "# Split training data into train and validation sets\n",
    "# Define possible hyperparameters\n",
    "# Create DataLoaders for train, validation, and test datasets\n",
    "# number of epochs defined \n",
    "# Initialize model, loss function, and optimizer\n",
    "# Dictionaries to track the best configurations\n",
    "# Combined best configuration that stores all best hyperparameters\n",
    "# Set device to GPU if available, otherwise use CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T22:06:53.795334Z",
     "start_time": "2024-11-29T22:06:53.780886Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
    "    model.to(device)\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.view(-1, 1, 28, 28)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "        train_losses.append(train_loss / total)\n",
    "        train_accuracies.append(correct / total)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.view(-1, 1, 28, 28)\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "        val_losses.append(val_loss / val_total)\n",
    "        val_accuracies.append(val_correct / val_total)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"  Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}\")\n",
    "        print(f\"  Val Loss: {val_losses[-1]:.4f}, Val Accuracy: {val_accuracies[-1]:.4f}\")\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function definition\n",
    "# defined lists to keep track of losses and accuracies \n",
    "# loop thorugh all training data \n",
    "# forward pass then compute the loss\n",
    "# backwardpropagation \n",
    "# update weights\n",
    "# count correctly predicted \n",
    "# get training accuarcy and loss\n",
    "# reapet validation data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T22:39:34.183255Z",
     "start_time": "2024-11-29T22:39:34.176471Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.view(-1, 1, 28, 28)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = correct / total\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finally using best configuration retrain the model\n",
    "# then test on the test data\n",
    "# plot all graphs and the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T22:06:55.095413Z",
     "start_time": "2024-11-29T22:06:55.087714Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    # Plotting the training and validation loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)  # 1 row, 2 columns, first subplot\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)  # 1 row, 2 columns, second subplot\n",
    "    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    # Show the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T22:39:34.164972Z",
     "start_time": "2024-11-29T22:06:55.752534Z"
    }
   },
   "outputs": [],
   "source": [
    "for lr in learning_rates:\n",
    "    mymodel = CNNModel()\n",
    "    criterion =nn.CrossEntropyLoss()\n",
    "    print(f\"Training with learning rate: {lr}\")\n",
    "    optimizer = torch.optim.SGD(mymodel.parameters(), lr=lr)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(mymodel, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
    "    plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \n",
    "    max_val_accuracy = max(val_accuracies)\n",
    "    \n",
    "    # Update the best learning rate configuration\n",
    "    if max_val_accuracy > best_lr_config[\"val_accuracy\"]:\n",
    "        best_lr_config.update({\"learning_rate\": lr, \"val_accuracy\": max_val_accuracy})\n",
    "    if max_val_accuracy > combined_best_config[\"val_accuracy\"]:\n",
    "        combined_best_config.update({\"learning_rate\": lr, \"batch_size\": 64, \"filter_config\": [32, 64, 128], \"val_accuracy\": max(val_accuracies)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T23:04:48.426307Z",
     "start_time": "2024-11-29T22:39:59.788132Z"
    }
   },
   "outputs": [],
   "source": [
    "for bs in batch_sizes:\n",
    "    mymodel = CNNModel()\n",
    "    criterion =nn.CrossEntropyLoss()\n",
    "    optimize=torch.optim.SGD(mymodel.parameters(),lr=0.01)\n",
    "    print(f\"Training with batch size: {bs}\")\n",
    "    train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(mymodel, train_loader, val_loader, criterion, optimize, epochs, device)\n",
    "    plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \n",
    "    max_val_accuracy = max(val_accuracies)\n",
    "    \n",
    "    # Update the best batch size configuration\n",
    "    if max_val_accuracy > best_bs_config[\"val_accuracy\"]:\n",
    "        best_bs_config.update({\"batch_size\": bs, \"val_accuracy\": max_val_accuracy})\n",
    "    if max_val_accuracy > combined_best_config[\"val_accuracy\"]:\n",
    "        combined_best_config.update({\"learning_rate\": 0.01, \"batch_size\": bs, \"filter_config\": [32, 64, 128], \"val_accuracy\": max(val_accuracies)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fs in filter_config:\n",
    "    print(f\"Training with filter configuration: {fs}\")\n",
    "    mymodel=CNNModel(filters=fs)\n",
    "    criterion =nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.01)\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies = train_model(mymodel, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
    "    plot_loss_accuracy(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    max_val_accuracy = max(val_accuracies)\n",
    "    if max_val_accuracy > best_filter_config[\"val_accuracy\"]:\n",
    "        best_filter_config.update({\"filter_config\": fs, \"val_accuracy\": max_val_accuracy})\n",
    "    if max_val_accuracy > combined_best_config[\"val_accuracy\"]:\n",
    "        combined_best_config.update({\"learning_rate\": 0.01, \"batch_size\": 64, \"filters\": fs, \"val_accuracy\": max_val_accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-29T23:05:33.329943Z",
     "start_time": "2024-11-29T23:05:10.180452Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Re-train and evaluate the final combined best model\n",
    "print(\"\\nTraining and evaluating the best combined model on test set...\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=combined_best_config[\"batch_size\"], shuffle=True)\n",
    "mymodel = CNNModel(combined_best_config[\"filter_config\"]) \n",
    "optimizer = torch.optim.SGD(mymodel.parameters(), lr=combined_best_config[\"learning_rate\"])\n",
    "train_model(mymodel, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
    "evaluate_model(mymodel, test_loader,device)\n",
    "\n",
    "# Print the best configurations\n",
    "print(\"\\nBest Configurations:\")\n",
    "print(f\"  Best Learning Rate: {best_lr_config['learning_rate']} (Validation Accuracy: {best_lr_config['val_accuracy']:.4f})\")\n",
    "print(f\"  Best Batch Size: {best_bs_config['batch_size']} (Validation Accuracy: {best_bs_config['val_accuracy']:.4f})\")\n",
    "print(f\"  Best filter config:{best_filter_config['filter_config']} (Validation Accuracy: {best_filter_config['val_accuracy']:.4f})\")\n",
    "print(f\"  Combined Best Configuration: {combined_best_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Size\n",
    "#   The batch size determines how many training samples the model processes before updating the weights during training\n",
    "#   small batch size --->More frequent weight updates and Better generalization Using small batches introduces more noise into the gradient estimation  which can help the model avoid overfitting  and lead to better generalization\n",
    "# but Slower training and Noisy gradient updates are cons \n",
    "#   large batch size ---->Stable gradient updates as Larger batches tend to produce more stable and accurate gradient estimates since they average over a larger number of samples and More efficient hardware utilization as  With larger batches modern hardware like GPUs can process more data in parallel\n",
    "# but Slower convergence and Risk of overfitting are cons \n",
    "# learning rate : controls how much model's wieght are adjusted\n",
    "# small---->More precise convergence and stable but Slow convergence and Risk of getting stuck in local minima\n",
    "# large faster convergence but poor generalization and can lead to overfitting  \n",
    "# number of filters:in CNNs filters  are responsible for detecting patterns in the input images the number of filters determines the modelâ€™s capacity to learn complex patterns\n",
    "# small -----> Smaller numbers of filters result in fewer weights  which can reduce the computational cost and memory requirements allowing for faster training but The model may not be able to capture complex patterns in the data especially for tasks like image classification where high-level features are crucial and may unerfit \n",
    "# large Increased model capacity so it can learn more complex features and Better feature extraction but Increased computational cost and may overfit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
